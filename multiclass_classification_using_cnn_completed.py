# -*- coding: utf-8 -*-
"""Multiclass_classification_using_CNN_Completed.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NBy-naXrMF1Z-gMG8F7tzR0J1-Z8Te25
"""

from google.colab import drive
drive.mount("/content/gdrive")

# Import libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import tensorflow as tf
import keras
from keras import layers
from keras.preprocessing import image
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import Conv2D
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

import os
import shutil
import warnings
warnings.filterwarnings('ignore')

"""# Visualise the Images"""

# Let's plot a few images
train_path = "/content/gdrive/MyDrive/Datasets/Vegetable Images/train"
validation_path = "/content/gdrive/MyDrive/Datasets/Vegetable Images/validation"
test_path = "/content/gdrive/MyDrive/Datasets/Vegetable Images/test"

image_categories = os.listdir('/content/gdrive/MyDrive/Datasets/Vegetable Images/train')

def plot_images(image_categories):
    
    # Create a figure
    plt.figure(figsize=(12, 12))
    for i, cat in enumerate(image_categories):
        
        # Load images for the ith category
        image_path = train_path + '/' + cat
        images_in_folder = os.listdir(image_path)
        first_image_of_folder = images_in_folder[0]
        first_image_path = image_path + '/' + first_image_of_folder
        img = image.load_img(first_image_path)
        img_arr = image.img_to_array(img)/255.0
        
        # Create Subplot and plot the images
        plt.subplot(4, 4, i+1)
        plt.imshow(img_arr)
        plt.title(cat)
        plt.axis('off')
        
    plt.show()

# Call the function
plot_images(image_categories)

"""# Prepare the Dataset"""

# Creating Image Data Generator for train, validation and test set

# 1. Train Set
train_gen = ImageDataGenerator(rescale = 1.0/255.0) # Normalise the data
train_image_generator = train_gen.flow_from_directory(
                                            train_path,
                                            target_size=(150, 150),
                                            batch_size=32,
                                            class_mode='categorical',
                                            shuffle = True)

# 2. Validation Set
val_gen = ImageDataGenerator(rescale = 1.0/255.0) # Normalise the data
val_image_generator = train_gen.flow_from_directory(
                                            validation_path,
                                            target_size=(150, 150),
                                            batch_size=32,
                                            class_mode='categorical',
                                            shuffle = True)

# 3. Test Set
test_gen = ImageDataGenerator(rescale = 1.0/255.0) # Normalise the data
test_image_generator = train_gen.flow_from_directory(
                                            test_path,
                                            target_size=(150, 150),
                                            batch_size=32,
                                            class_mode='categorical',
                                            shuffle = True)

train = {}
for a in os.listdir(train_path):
  train[a]=len(os.listdir(train_path+"/"+a))
test = {}
for a in os.listdir(test_path):
  test[a]=len(os.listdir(test_path+"/"+a))
validation = {}
for a in os.listdir(validation_path):
  validation[a]=len(os.listdir(validation_path+"/"+a))

print("Train\n",train)
print("Test\n",test)
print("Validation\n",validation)

df=pd.DataFrame.from_dict(train.items())
df.plot(kind="bar",color='c')
plt.xlabel("Class Number")
plt.ylabel("Number of Samples")
plt.title("Training Data")

df1=pd.DataFrame.from_dict(test.items())
df1.plot(kind="bar",color='y')
plt.xlabel("Class Number")
plt.ylabel("Number of Samples")
plt.title("Testing Data")

df2=pd.DataFrame.from_dict(validation.items())
df2.plot(kind="bar",color='b')
plt.xlabel("Class Number")
plt.ylabel("Number of Samples")
plt.title("Validation Data")

# Print the class encodings done by the generators
class_map = dict([(v, k) for k, v in train_image_generator.class_indices.items()])
print(class_map)

"""# Building a CNN model"""

# Build a custom sequential CNN model

model = Sequential() # model object

# Add Layers
model.add(layers.Conv2D(filters=32, kernel_size=(3,3), strides=1, padding='same', activation='relu', input_shape=[150, 150, 3]))
model.add(layers.MaxPooling2D(2,2))
model.add(layers.Conv2D(filters=64, kernel_size=(3,3), strides=1, padding='same', activation='relu'))
model.add(layers.MaxPooling2D(2,2))
model.add(layers.Conv2D(filters=128, kernel_size=(3,3), strides=1, padding='same', activation='relu'))
model.add(layers.MaxPooling2D(2,2))

# Flatten the feature map
model.add(layers.Flatten())

# Add the fully connected layers
model.add(layers.Dense(128,activation="relu"))
model.add(layers.Dropout(0.25))
model.add(layers.Dense(128, activation='relu'))
model.add(layers.Dense(7, activation='softmax'))

# print the model summary
model.summary()

# Compile and fit the model
early_stopping = keras.callbacks.EarlyStopping(patience=5) # Set up callbacks
model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics='accuracy')
hist = model.fit(train_image_generator, 
                 epochs=25, 
                 verbose=1, 
                 batch_size=1000,
                 validation_data=val_image_generator, 
                 steps_per_epoch = 7000//1000, 
                 validation_steps = 1400//1000, 
                 callbacks=early_stopping)

# Plot the error and accuracy
h = hist.history
plt.style.use('bmh')
plt.figure(figsize=(10, 5))
plt.plot(h['loss'], c='red',linewidth=3, label='Training Loss')
plt.plot(h['val_loss'], c='blue',linewidth=3, linestyle='--', label='Validation Loss')
plt.xlabel("Number of Epochs")
plt.legend(loc='best')
plt.show()

h = hist.history
plt.style.use('dark_background')
plt.figure(figsize=(10, 5))
plt.plot(h['accuracy'], c='yellow',linewidth=3, label='Training Accuracy')
plt.plot(h['val_accuracy'], c='red',linewidth=3, linestyle='--', label='Validation Accuracy')
plt.xlabel("Number of Epochs")
plt.legend(loc='best')
plt.show()

# Predict the accuracy for the test set
model.evaluate(test_image_generator)

y_test = test_image_generator.classes
y_pred = model.predict(test_image_generator)
y_pred = np.argmax(y_pred, axis=1)
y_pred

conf_mat = confusion_matrix(y_test, y_pred)
conf_mat = conf_mat / conf_mat.astype(np.float).sum(axis=1)
sns.heatmap(conf_mat, square=True, annot=True, cmap='Blues', cbar=False)

print(classification_report(y_test,y_pred))

h = hist.history
df_1 = pd.DataFrame((h["accuracy"],h["val_accuracy"],h["loss"],h["val_loss"])).T
df_1.columns = ["Acc" , "Val_Acc","Loss","Val_Loss"]
df_1

df_1.to_csv("Model_History.csv")

# Build a custom sequential CNN model

model_2 = Sequential() # model object

# Add Layers
model_2.add(layers.Conv2D(filters=32, kernel_size=(5,5), strides=1, padding='valid', activation='relu', input_shape=[150, 150, 3]))
model_2.add(layers.MaxPooling2D(2,2))
model_2.add(layers.Conv2D(filters=64, kernel_size=(5,5), strides=1, padding='valid', activation='relu'))
model_2.add(layers.MaxPooling2D(2,2))
model_2.add(layers.Conv2D(filters=128, kernel_size=(5,5), strides=1, padding='valid', activation='relu'))
model_2.add(layers.MaxPooling2D(2,2))


# Flatten the feature map
model_2.add(layers.Flatten())

# Add the fully connected layers
model_2.add(layers.Dense(512, activation='relu'))
model_2.add(layers.Dense(7, activation='softmax'))

# print the model summary
model_2.summary()

# Compile and fit the model
early_stopping = keras.callbacks.EarlyStopping(patience=5) # Set up callbacks
model_2.compile(optimizer='Adam', loss='categorical_crossentropy', metrics='accuracy')
hist_2 = model_2.fit(train_image_generator, 
                 epochs=25, 
                 verbose=1, 
                 batch_size=1000,
                 validation_data=val_image_generator, 
                 steps_per_epoch = 7000//1000, 
                 validation_steps = 1400//1000, 
                 callbacks=early_stopping)

# Plot the error and accuracy
h = hist_2.history
plt.style.use('seaborn-whitegrid')
plt.figure(figsize=(10, 5))
plt.plot(h['loss'], c='red',linewidth=3, label='Training Loss')
plt.plot(h['val_loss'], c='blue',linewidth=3, linestyle='--', label='Validation Loss')
plt.xlabel("Number of Epochs")
plt.legend(loc='best')
plt.show()

h = hist_2.history
plt.style.use('seaborn-whitegrid')
plt.figure(figsize=(10, 5))
plt.plot(h['accuracy'], c='blue',linewidth=3, label='Training Accuracy')
plt.plot(h['val_accuracy'], c='red',linewidth=3, linestyle='--', label='Validation Accuracy')
plt.xlabel("Number of Epochs")
plt.legend(loc='best')
plt.show()

# Predict the accuracy for the test set
model_2.evaluate(test_image_generator)

y_test = test_image_generator.classes
y_pred = model_2.predict(test_image_generator)
y_pred = np.argmax(y_pred, axis=1)
y_pred

conf_mat = confusion_matrix(y_test, y_pred)
conf_mat = conf_mat / conf_mat.astype(np.float).sum(axis=1)
sns.heatmap(conf_mat, square=True, annot=True, cmap='Blues', cbar=False)

print(classification_report(y_test,y_pred))

# Build a custom sequential CNN model

model_3 = Sequential() # model object

# Add Layers
model_3.add(layers.Conv2D(filters=32, kernel_size=(3,3), strides=2, padding='same', activation='relu', input_shape=[150, 150, 3]))
model_3.add(layers.MaxPooling2D(2,2))
model_3.add(layers.Conv2D(filters=64, kernel_size=(5,5), strides=2, padding='same', activation='relu'))
model_3.add(layers.MaxPooling2D(2,2))
model_3.add(layers.Conv2D(filters=128, kernel_size=(3,3), strides=2, padding='same', activation='relu'))
model_3.add(layers.MaxPooling2D(2,2))


# Flatten the feature map
model_3.add(layers.Flatten())

# Add the fully connected layers
model_3.add(layers.Dense(256, activation='relu'))
model_3.add(layers.Dense(7, activation='softmax'))

# print the model summary
model_3.summary()

# Compile and fit the model
early_stopping = keras.callbacks.EarlyStopping(patience=5) # Set up callbacks
model_3.compile(optimizer='SGD', loss='categorical_crossentropy', metrics='accuracy')
hist_3 = model_3.fit(train_image_generator, 
                 epochs=25, 
                 verbose=1, 
                 batch_size=1000,
                 validation_data=val_image_generator, 
                 steps_per_epoch = 7000//1000, 
                 validation_steps = 1400//1000, 
                 callbacks=early_stopping)

# Plot the error and accuracy
h = hist_3.history
plt.style.use('dark_background')
plt.figure(figsize=(10, 5))
plt.plot(h['loss'], c='red',linewidth=3, label='Training Loss')
plt.plot(h['val_loss'], c='blue',linewidth=3, linestyle='--', label='Validation Loss')
plt.xlabel("Number of Epochs")
plt.legend(loc='best')
plt.show()

h = hist_3.history
plt.style.use('Solarize_Light2')
plt.figure(figsize=(10, 5))
plt.plot(h['accuracy'], c='blue',linewidth=3, label='Training Accuracy')
plt.plot(h['val_accuracy'], c='red',linewidth=3, linestyle='--', label='Validation Accuracy')
plt.xlabel("Number of Epochs")
plt.title("Accuracy with Optimizer=SGD")
plt.legend(loc='best')
plt.show()

# Predict the accuracy for the test set
model_3.evaluate(test_image_generator)

y_test = test_image_generator.classes
y_pred = model_3.predict(test_image_generator)
y_pred = np.argmax(y_pred, axis=1)
y_pred

conf_mat = confusion_matrix(y_test, y_pred)
conf_mat = conf_mat / conf_mat.astype(np.float).sum(axis=1)
sns.heatmap(conf_mat, square=True, annot=True, cmap='Blues', cbar=False)

print(classification_report(y_test,y_pred))

# Testing the Model
test_image_path = '/content/gdrive/MyDrive/Datasets/Vegetable Images/test/Carrot/1070.jpg'

def generate_predictions(test_image_path, actual_label):
    
    # 1. Load and preprocess the image
    test_img = image.load_img(test_image_path, target_size=(150, 150))
    test_img_arr = image.img_to_array(test_img)/255.0
    test_img_input = test_img_arr.reshape((1, test_img_arr.shape[0], test_img_arr.shape[1], test_img_arr.shape[2]))

    # 2. Make Predictions
    predicted_label = np.argmax(model.predict(test_img_input))
    predicted_vegetable = class_map[predicted_label]
    plt.figure(figsize=(4, 4))
    plt.imshow(test_img_arr)
    plt.title("Predicted Label: {}, Actual Label: {}".format(predicted_vegetable, actual_label))
    plt.grid()
    plt.axis('off')
    plt.show()

# call the function
generate_predictions(test_image_path, actual_label='Carrot')

!wget "https://www.dropbox.com/s/lge1plvr4mg5w7y/potato_2.jpg?dl=0"

# Generate predictions for external image
external_image_path_2 = "./potato_2.jpg?dl=0"
generate_predictions(external_image_path_2, actual_label='Potato')

model.save("/content/gdrive/MyDrive/model_1.h5")

model_2.save("/content/gdrive/MyDrive/model_2.h5")

model_3.save("/content/gdrive/MyDrive/model_3.h5")

